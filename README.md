# Async Web Crawler

An asynchronous Python web crawler that:
- Crawls pages with controlled concurrency
- Extracts headings, paragraphs, links, and images
- Normalizes URLs and avoids duplicates
- Saves results into a CSV report

- 
## Requirements

- Python 3.10+
- uv (for running the project)
- aiohttp
- beautifulsoup4

## Usage

Run the crawler with:

# Until changed for prompting, you must execute it with:
  # uv run main.py <URL> <max_concurrency> <max_pages>

# after update, you will be prompted for these 3 things.

# future updates will include word search prompt(s) 
